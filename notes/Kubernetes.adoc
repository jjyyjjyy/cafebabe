= Kubernetes
:icons: font
:sectanchors:
:page-layout: docs
:toc: left
:toc-title: 索引

== 1. 系统配置:

[source,bash]
----
systemctl disable firewalld
systemctl stop firewalld
sudo vim /etc/sysconfig/selinux -> SELINUX=disabled
# 关闭交换分区
sudo vim /etc/fstab -> 注释/swapfile行
sudo mount -a
sudo swapoff -a
----

== 2. 安装kubeadm/kubectl/kubelet:

[source,bash]
----
include::sh/k8s-install.sh[]
----

== 3. 安装

=== 3.1 使用kubeadm创建Kubernetes集群
[source,bash]
----
sudo kubeadm init --kubernetes-version=1.11.3 --pod-network-cidr=10.244.0.0/16
# 安装成功后,创建kubectl配置文件
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# network插件
kubectl apply -f https://cloud.weave.works/k8s/v1.8/net.yaml

# kube单机 (Optional)
kubectl taint nodes --all node-role.kubernetes.io/master-

# check finally
kubectl get nodes
kubectl get pods --all-namespaces

# Trouble shooting:
# kubectl -n kube-system describe TYPE NAME

# 清除安装:
include::sh/k8s-reset.sh[]

# sh helper:
alias kn='kubectl get nodes -o wide'
alias kp='kubectl get pods --all-namespaces -o wide'
alias kc='kubectl create -f'
alias ka='kubectl apply -f'
alias kr='kubectl replace -f'

----

=== 3.2 minikube (Optional)

[source,bash]
----
sudo apt install virtualbox
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.0/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
minikube start --registry-mirror=https://registry.docker-cn.com
minikube dashboard

# 删除原有minikube
minikube delete && rm -rf ~/.minikube
----

== 4. 插件安装
=== 4.1 安装Dashboard

[source,bash]
----
kubectl create -f http://soft-1252259164.file.myqcloud.com/dashboard.yml
----
=== 4.2 创建登陆ServiceAccount
[source,bash]
----
cat <<EOF > sa-dashboard.yml
apiVersion: v1
kind: ServiceAccount
metadata:
    name: admin-user
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
    name: admin-user
roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF

kubectl create -f sa-dashboard.yml
# 获取登陆token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

# 浏览器访问
# https://ip:30001
----

=== 4.3 安装Heapster插件

[source,bash]
----
mkdir -p heapster
cd heapster
wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml
wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml
wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml
sed -i s/"k8s.gcr.io"/"registry.cn-hangzhou.aliyuncs.com\/google_containers"/g *.*
cd ..
kubectl create -f heapster/
----

=== 4.4 Helm

[source,bash]
----
curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
chmod 700 get_helm.sh
./get_helm.sh
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.11.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'

----

=== 4.5 启用api-server的swagger ui

[source,bash]
----
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
# command 加上
- --enable-swagger-ui=true
- --insecure-bind-address=0.0.0.0
- --insecure-port=8080
----

TIP: api-server属于static pod, 改完配置文件后会自动生效.

== 5. kubernetes各个组件功能

=== 5.1 Master
集群控制节点,运行以下进程:

* kube-apiserver: 提供Kubernetes所有资源操作的REST服务
* kube-controller-manager: 自动化控制所有资源对象
* kube-scheduler: pod调度
* kube-proxy: 实现service通信个负载均衡
* etcd: 保存资源数据

=== 5.2 Node

集群工作负载节点,运行以下进程:

* kubelet: 负责pod里容器的创建、启动等任务
* kube-proxy: 实现service通信个负载均衡
* pause: pod组件
* docker: 负责本机的容器管理

=== 5.3 Pod
每个pod包含pause容器和多个用户容器.
同一个pod内的容器拥有相同的Network/IPC/UTS[和PID] namespace, 所以它们拥有相同的hostname和network interfaces,但文件系统不同

* static pod: 设置在node中,并只在该node上运行
* normal pod: 存储在etcd中,随后会被Master调度到某个node上进行绑定,node上的kubelet进程会实例化一组docker容器运行

=== 5.4 Label Selector
* kube-controller进程通过RC上定义的labelSelector来筛选需要监控的Pod数量.
* kube-proxy进程通过service的labelSelector来选择对应的pod, 自动建立起每个service到对应pod的请求路由, 从而实现service的负载均衡.
* kube-scheduler通过node定义的label和pod定义nodeSelector实现pod的定向调度.

=== 5.5 ReplicationController

 定期监控labelSelector匹配的pod副本的数量,确保其等于期望值.

WARNING: 如果修改了一个pod的label, 则RC会自动新起一个pod.如果修改了RC的labelSelector, 则RC会自动创建所有pod副本.

=== 5.6 Replica Set

 包含ReplicationController所有的功能, 同时增强了labelSelector的功能.

=== 5.7 DaemonSet

 通过nodeSelector在每个节点上运行一个实例

=== 5.8 Job

 启动批处理Job, 可以设置任务数/并行数/重试次数等

=== 5.9 CronJob

 定时任务: 分/时/日/月/星期

=== 5.10 Deployment

 扩展了Replica Set自动scale的功能.

== 6. kubectl 命令

[source,bash]
----
kubectl version
kubectl cluster-info
kubectl api-versions
kubectl config view
kubectl get [po|depoy|cm]

# namespace
kubectl create namespace xxNameSpace
# label
kubectl label po xxPod xxx=xxx [--override]
# annotation
kubectl annotate po xxPod xxx=xxx
# label selector
kubectl [get|delete] po -l xxx=xxx

# ======资源控制======
kubectl create -f YML

# configMap
kubectl create cm CM_NAME --from-file=xxxFile   # xxxFile=CONTENT
kubectl create cm CM_NAME --from-file=xxxDir   # xxxFile1=CONTENT1, xxxFile2=CONTENT2
kubectl create cm CM_NAME --from-literal=a1=v1 --from-literal=a2=v2

kubectl run NAME --image=IMAGE

kubectl delete TYPE NAME

# ======部署管理======
# 实现水平扩展
kubectl scale rc <rc_name> [-n <namespace>] --replicas=2
# 部署状态变更状态检查
kubectl rollout status
# 部署历史
kubectl rollout history
# 回滚部署
kubectl rollout undo
----

== 7. Pod

=== 7.1 生命周期:

. Pending `Pod创建成功,但存在容器还未创建`
. Running `Pod中容器都已成功创建`
. Succeeded `Pod中所有容器均已创建成功`
. Failed `Pod中所有容器均已退出,并且至少有一个退出为失败状态`
. Unknown `无法获取Pod的状态`

=== 7.2 重启策略
* Always `容器失效时kubelet自动重启容器`
* OnFailure `容器停止运行且退出码不为0时kubelet自动重启容器`
* Never `kubelet不会自动重启容器`

 RC和DS必须设置为Always
 Job必须设置为OnFailure或Never

=== 7.3 健康检查
* livenessProbe (running)

 如果探测到容器不健康, 则kubelet将杀掉该容器, 并根据容器的重启策略处理

.实现方式
* ExecAction
* TCPSocketAction
* HTTPGetAction
[source,yml]
----
  livenessProbe:
    httpGet:
      port: 8080
      path: /actuator/health
    initialDelaySeconds:10 # 首次进行健康检查的延时时长,单位为秒
    timeoutSeconds:2 # 端点超时时长
    periodSeconds:10 # 定时任务
----

* readinessProbe (ready)

 请求容器, 如果容器不健康, Endpoint Controller 将从Service的Endpoint中删除该Pod的Endpoint

=== 7.4 调度

* Deployment 自动调度
* NodeSelector 定向调度

 给node打标签: kubectl label node <node-name> <label-key>=<label-value>
 yml配置label: spec.template.spec.nodeSelector

* NodeAffinity Node亲和力调度
** RequiredDuringSchedulingIgnoredDuringExecution

 必须满足指定的规则才可以调度Pod到Node上

** PreferredDuringSchedulingIgnoredDuringExecution

 优先满足指定规则即可

 spec.affinity.nodeAffinity 有一个条件满足即可, spec.affinity..nodeSelectorTerms必须满足所有条件

* PodAffinity Pod亲和力调度
* Taint & toleration

 kubectl taint nodes <node-name> k=v:NoSchedule

* DaemonSet 每个node调度1个Pod
* Job 批处理调度
* Cronjob 定时任务

=== 7.5 升级和回滚

==== 升级

 * kubectl set image deployment/<deployment-name> <container-name>=<image-name:tag>
 * kubectl edit deployment/<deployment-name>

可以通过spec.strategy.type确定升级方式: RollingUpdate/Recreate

==== 回滚
[%hardbreaks]
查看升级状态: `kubectl rollout status deployment/<deployment-name>`
查看升级历史: `kubectl rollout history deployment/<deployment-name> 创建deployment时加上--record参数`
回滚到指定版本 `kubectl rollout undo deployment/<deployment-name> --to-revision=<revision>`
暂停/继续更新: `kubectl rollout pause/resume deployment/<deployment-name>`

=== 7.6 扩容和缩容

 kubectl scale deployment <deployment-name> --replicas <number>

== 8. Service

=== 8.1 基本用法:

[source,yml]
.springboot-svc.yml
----
apiVersion: v1
kind: Service
metadata:
    name: springboot-demo
spec:
    selector:
        app: springboot-demo
    ports:
    - port: 8081 #service暴露的端口
      targetPort: 8080 #pod端口
----

 kubectl create -f springboot-svc.yml

=== 8.2 暴露外部端口

* 设置容器级别的hostPort, 将容器应用的端口号映射到物理机上

[source,yml]
----
ports:
- containerPort: 8080
  hostPort: 8081
----

* Pod设置hostNetwork=true, 容器中的端口号被映射到物理机上
* Service设置nodePort映射到物理机,同时设置service的类型为NodePort

 service的nodePort默认范围为30000-32767,可以修改/etc/kubernetes/manifests/kube-apiserver.yaml 添加command `- --service-node-port-range=80-32767`

[source,yml]
----
type: NodePort
ports:
- port: 8080
  targetPort: 8080
  nodePort: 8081
----

* 设置service的spec.type为LoadBalancer, 不需要设置NodePort
* Ingress

== 9. Helm

 Kubernetes 包管理工具

=== 9.1 基本概念

* Chart: chart用于描述一个安装包
* Repository: 存放chart的database
* Release: 包安装后的一个实例

=== 9.2 https://docs.helm.sh/developing_charts/#charts[常用命令]

 helm search <chart>
 helm inspect [values] <chart>
 helm ls
 helm install {CHART|TAR|URL} [--name <name>]
 helm delete <chart>
 helm repo list
 helm repo add dev https://example.com/dev-charts
 helm create <new_chart>
 helm package <chart>

== 10. Istio

=== 10.1 核心功能

 ServiceMesh解决方案

* Envoy: 调节服务流量
* Mixer: 执行访问控制和使用策略, 并从Envoy代理和其他服务收集数据
* Pilot: 为Envoy提供服务发现功能
* Citadel: 身份认证

image::https://istio.io/docs/concepts/what-is-istio/arch.svg[]

=== 10.2 安装

[source,bash]
----
curl -L https://git.io/getLatestIstio | sh -
mv istio-* istio/
# bash修改PATH环境变量: export PATH=$HOME/istio/bin:$PATH
cd istio/
kubectl create namespace istio-system
helm template install/kubernetes/helm/istio --name istio --namespace istio-system > $HOME/istio.yaml
kubectl create -f $HOME/istio.yaml
----

== 技能图谱

image::https://static001.geekbang.org/resource/image/90/77/90af293d7ff8caa58f5ac458a3b03577.jpg[]
