= Kubernetes
:icons: font
:source-highlighter: highlightjs
:highlightjs-theme: idea
:sectlinks:
:sectnums:
:stem:
:toc: left
:toclevels: 3
:toc-title: 目录
:tabsize: 4
:docinfo: shared

== 系统配置:

[source,bash]
----
systemctl disable firewalld
systemctl stop firewalld
sudo vim /etc/sysconfig/selinux -> SELINUX=disabled
# 关闭交换分区
sudo vim /etc/fstab -> 注释/swapfile行
sudo mount -a
sudo swapoff -a
----

== 安装

=== 使用kubeadm安装

[source,bash]
----
include::sh/k8s-install.sh[]

sudo kubeadm init --kubernetes-version=`kubeadm version -o short` --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all
# 安装成功后,创建kubectl配置文件
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# network插件
kubectl apply -f https://cloud.weave.works/k8s/v1.10/net.yaml
# bash补全
source <(kubectl completion bash)

# kube单机 (Optional)
# kubectl taint nodes --all node-role.kubernetes.io/master-

# check finally
kubectl get nodes
kubectl get pods --all-namespaces

# Trouble shooting:
# kubectl -n kube-system describe TYPE NAME

# 清除安装:
# sudo kubeadm reset -f
# rm -rf ~/.kube

# sh helper:
alias kn='kubectl get nodes -o wide'
alias kp='kubectl get pods --all-namespaces -o wide'
alias kc='kubectl create -f'
alias ka='kubectl apply -f'
alias kr='kubectl replace -f'
alias kt="kubectl -n kube-system describe secret `kubectl -n kube-system get secret | grep admin-user | awk '{print $1}'`"

----

=== 使用Vagrant安装

[source,ruby]
.Vagrantfile
----
include::k8s/vagrant/Vagrantfile[]
----

[source,bash]
.init.sh
----
include::k8s/vagrant/init.sh[]
----

[source,bash]
.kubeadm-init.sh
----
include::k8s/vagrant/kubeadm-init.sh[]
----

== 插件安装

=== 安装Dashboard

[source,bash]
----
mkdir certs
openssl req -nodes -newkey rsa:2048 -keyout certs/dashboard.key -out certs/dashboard.csr -subj "/C=/ST=/L=/O=/OU=/CN=kubernetes-dashboard"
openssl x509 -req -sha256 -days 10000 -in certs/dashboard.csr -signkey certs/dashboard.key -out certs/dashboard.crt
kubectl create -f https://soft-1252259164.cos.ap-shanghai.myqcloud.com/dashboard.yml
kubectl create secret generic kubernetes-dashboard-certs --from-file=certs -n kubernetes-dashboard
----

=== 启用api-server的swagger ui

[source,bash]
----
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
# command 加上
- --enable-swagger-ui=true
- --insecure-bind-address=0.0.0.0
- --insecure-port=8080
----

TIP: api-server属于static pod, 改完配置文件后会自动生效.

=== helm安装

[source,bash]
----
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
helm repo add stable http://mirror.azure.cn/kubernetes/charts/
----

== kubernetes各个组件功能

=== Master

集群控制节点,运行以下进程:

* kube-apiserver: 提供Kubernetes所有资源操作的REST服务
* kube-controller-manager: 自动化控制所有资源对象
* kube-scheduler: pod调度
* kube-proxy: 实现service通信个负载均衡
* etcd: 保存资源数据
* coredns

=== Node

集群工作负载节点,运行以下进程:

* kubelet: 负责pod里容器的创建、启动等任务
* kube-proxy: 实现service通信个负载均衡
* pause: pod组件
* docker: 负责本机的容器管理

=== Pod

每个pod包含pause容器和多个用户容器.
同一个pod内的容器拥有相同的Network/IPC/UTS[和PID] namespace, 所以它们拥有相同的hostname和network interfaces,但文件系统不同

* static pod: 设置在node中,并只在该node上运行
* normal pod: 存储在etcd中,随后会被Master调度到某个node上进行绑定,node上的kubelet进程会实例化一组docker容器运行

=== Label Selector

* kube-controller进程通过RC上定义的labelSelector来筛选需要监控的Pod数量.
* kube-proxy进程通过service的labelSelector来选择对应的pod, 自动建立起每个service到对应pod的请求路由, 从而实现service的负载均衡.
* kube-scheduler通过node定义的label和pod定义nodeSelector实现pod的定向调度.

=== ReplicationController

 定期监控labelSelector匹配的pod副本的数量,确保其等于期望值.

WARNING: 如果修改了一个pod的label, 则RC会自动新起一个pod.如果修改了RC的labelSelector, 则RC会自动创建所有pod副本.

=== Replica Set

 包含ReplicationController所有的功能, 同时增强了labelSelector的功能.

=== DaemonSet

 通过nodeSelector在每个节点上运行一个实例

=== Job

 启动批处理Job, 可以设置任务数/并行数/重试次数等

=== CronJob

 定时任务: 分/时/日/月/星期

=== Deployment

 扩展了Replica Set自动scale的功能.

== kubectl命令

[source,bash]
----
# 获取k8s版本号
kubectl version [-o short]
# 查看所有的api版本号
kubectl api-versions
# 查看 Config metadata
kubectl config view
# 查看集群资源概况
kubectl get [node|ns|po|depoy|cm]
# 查看指定pod中某个容器日志
kubectl logs POD -c CONTAINER
# 创建namespace
kubectl create namespace xxNameSpace

# 设置label
kubectl label po xxPod xxx=xxx [--override]
# 显示label
kubectl get pods --show-labels
# 筛选label
kubectl get pods --show-labels -l env=xxx
# 每个node都会有一些默认label: kubernetes.io/hostname=HOSTNAME

# 编辑
kubectl edit rc RC

# 删除
kubectl delete [TYPE | all] [--all] [-n NAMESPACE]
# 删除rc但保留所有pod
kubectl delete rc RC --cascade=false


# configMap
kubectl create cm CM_NAME --from-file=xxxFile   # xxxFile=CONTENT
kubectl create cm CM_NAME --from-file=xxxDir   # xxxFile1=CONTENT1, xxxFile2=CONTENT2
kubectl create cm CM_NAME --from-literal=a1=v1 --from-literal=a2=v2

# ======部署管理======
# 实现水平扩展
kubectl scale rc <rc_name> [-n <namespace>] --replicas=2
# 部署状态变更状态检查
kubectl rollout status
# 部署历史
kubectl rollout history
# 回滚部署
kubectl rollout undo
----

== Pod

=== 生命周期:

. Pending `Pod创建成功,但存在容器还未创建`
. Running `Pod中容器都已成功创建`
. Succeeded `Pod中所有容器均已创建成功`
. Failed `Pod中所有容器均已退出,并且至少有一个退出为失败状态`
. Unknown `无法获取Pod的状态`

=== 重启策略

* Always `容器失效时kubelet自动重启容器`
* OnFailure `容器停止运行且退出码不为0时kubelet自动重启容器`
* Never `kubelet不会自动重启容器`

 RC和DS必须设置为Always
 Job必须设置为OnFailure或Never

=== 健康检查

* livenessProbe (running)

 如果探测到容器不健康, 则kubelet将杀掉该容器, 并根据容器的重启策略处理

.实现方式
* HTTPGetAction
* TCPSocketAction
* ExecAction

[source,yml]
----
  livenessProbe:
    httpGet:
      port: 8080
      path: /actuator/health
    initialDelaySeconds:10 # 首次进行健康检查的延时时长,单位为秒
    timeoutSeconds:2 # 端点超时时长
    periodSeconds:10 # 定时任务
----

* readinessProbe (ready)

 请求容器, 如果容器不健康, Endpoint Controller 将从Service的Endpoint中删除该Pod的Endpoint

=== 调度

* Deployment 自动调度
* NodeSelector 定向调度

 给node打标签: kubectl label node <node-name> <label-key>=<label-value>
 yml配置label: spec.template.spec.nodeSelector

* NodeAffinity Node亲和力调度
** RequiredDuringSchedulingIgnoredDuringExecution

 必须满足指定的规则才可以调度Pod到Node上

** PreferredDuringSchedulingIgnoredDuringExecution

 优先满足指定规则即可

 spec.affinity.nodeAffinity 有一个条件满足即可, spec.affinity..nodeSelectorTerms必须满足所有条件

* PodAffinity Pod亲和力调度
* Taint & toleration

 kubectl taint nodes <node-name> k=v:NoSchedule

* DaemonSet 每个node调度1个Pod
* Job 批处理调度
* Cronjob 定时任务

=== 升级和回滚

==== 升级

* kubectl set image deployment/<deployment-name> <container-name>=<image-name:tag>
* kubectl edit deployment/<deployment-name>

可以通过spec.strategy.type确定升级方式: RollingUpdate/Recreate

==== 回滚

[%hardbreaks]
查看升级状态: `kubectl rollout status deployment/<deployment-name>`
查看升级历史: `kubectl rollout history deployment/<deployment-name> 创建deployment时加上--record参数`
回滚到指定版本 `kubectl rollout undo deployment/<deployment-name> --to-revision=<revision>`
暂停/继续更新: `kubectl rollout pause/resume deployment/<deployment-name>`

=== 扩容和缩容

 kubectl scale deployment <deployment-name> --replicas <number>

== Service

=== 基本用法:

[source,yml]
.springboot-svc.yml
----
apiVersion: v1
kind: Service
metadata:
    name: springboot-demo
spec:
    selector:
        app: springboot-demo
    ports:
    - port: 8081 #service暴露的端口
      targetPort: 8080 #pod端口
----

 kubectl create -f springboot-svc.yml

=== Pod间访问

* 每一个pod中都会有集群中所有service的host/port环境变量.
如 `KUBERNETES_SERVICE_HOST/KUBERNETES_SERVICE_PORT` 对应名为kubernetes的service

* 直接通过dns访问: SERVICE[.NAMESPACE.svc.cluster.local], 如 `kubia.jy-test.svc.cluster.local`

=== 暴露外部端口

* 设置容器级别的hostPort, 将容器应用的端口号映射到物理机上

[source,yml]
----
ports:
- containerPort: 8080
  hostPort: 8081
----

* Pod设置hostNetwork=true, 容器中的端口号被映射到物理机上
* Service设置nodePort映射到物理机,同时设置service的类型为NodePort

 service的nodePort默认范围为30000-32767,可以修改/etc/kubernetes/manifests/kube-apiserver.yaml 添加command `- --service-node-port-range=80-32767`

[source,yml]
----
type: NodePort
ports:
- port: 8080
  targetPort: 8080
  nodePort: 8081
----

* 设置service的spec.type为LoadBalancer, 不需要设置NodePort
* Ingress

== PV

当集群用户需要在其 pod 中使用持久化存储(PV)时, 他们首先创建持久卷声明 (Persistent VolumeClaim, 简称 PVC) 清单, 指定所需要的最低容量要求和访问模式, 然后用户将PV提交给Kubernetes apiserver, Kubernetes将找到可匹 配的PV并将其绑定到PVC。

== ConfigMap

=== 创建方式

* 命令行直接创建 `kubectl create configmap kubia-config --from-literal=interval=2 --from-literal=foo=bar`
* 从文件(夹)创建 `kubectl create configmap kubia-config --from-file=config.conf`
* yaml创建:

[source,yml]
.demo-cm.yml
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubia-cm
  namespace: jy-test
data:
  log.level: debug
----

=== 使用:

[source,yml]
----
envFrom:
  - configMapRef:
      name: kubia-cm
    prefix: log
----

== Helm

 Kubernetes 包管理工具

=== 基本概念

* Chart: chart用于描述一个安装包
* Repository: 存放chart的database
* Release: chart在Kubernetes集群中部署后的一个实例
* Tiller: Helm的服务端, 用于接收Helm的请求, 根据Chart生成Kubernetes的manifest文件后再部署应用.

=== 安装

[source,bash]
----
sudo wget https://soft-1252259164.cos.ap-shanghai.myqcloud.com/helm -O /usr/local/bin/helm
sudo chmod +x /usr/local/bin/helm
kubectl -n kube-system create sa tiller
kubectl -n kube-system create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account tiller
----

=== 常用命令

https://docs.helm.sh/developing_charts/#charts

 helm search <chart>
 helm inspect [values] <chart>
 helm ls
 helm install {CHART|TAR|URL} [--name <name>]
 helm delete <chart>
 helm repo list
 helm repo add dev https://example.com/dev-charts
 helm create <new_chart>
 helm package <chart>

== Istio

=== 核心功能

 ServiceMesh解决方案

* Envoy: 调节服务流量
* Mixer: 执行访问控制和使用策略, 并从Envoy代理和其他服务收集数据
* Pilot: 为Envoy提供服务发现功能
* Citadel: 身份认证

image::https://istio.io/docs/concepts/what-is-istio/arch.svg[]

=== 安装

[source,bash]
----
curl -L https://git.io/getLatestIstio | sh -
mv istio-* istio/
# bash修改PATH环境变量: export PATH=$HOME/istio/bin:$PATH
cd istio/
kubectl create namespace istio-system
helm template install/kubernetes/helm/istio --name istio --namespace istio-system > $HOME/istio.yaml
kubectl create -f $HOME/istio.yaml
----

== 技能图谱

image::https://static001.geekbang.org/resource/image/90/77/90af293d7ff8caa58f5ac458a3b03577.jpg[]
